{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!pip install nltk"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\liana\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in c:\\users\\liana\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\liana\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\liana\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\liana\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.62.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\liana\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "from typing import List\r\n",
    "\r\n",
    "\r\n",
    "def summarize(text: str) -> str:\r\n",
    "    # Tokenize the text by sentences\r\n",
    "    sentences = nltk.sent_tokenize(text)\r\n",
    "\r\n",
    "    # Calculate the scores of each sentence\r\n",
    "    sentence_scores = _calculate_sentence_scores(\r\n",
    "        sentences, _create_dictionary_table(text))\r\n",
    "\r\n",
    "    # Get the average score\r\n",
    "    average_scores = _calculate_average_score(sentence_scores)\r\n",
    "\r\n",
    "    # Compare each sentence to the average score\r\n",
    "    # and get the highest score sentences\r\n",
    "    summary = _get_article_summary(sentences, sentence_scores, average_scores)\r\n",
    "\r\n",
    "    return summary\r\n",
    "\r\n",
    "\r\n",
    "def _create_dictionary_table(text: str) -> dict:\r\n",
    "\r\n",
    "    # Removing stop words\r\n",
    "    stop_words = set(stopwords.words(\"english\"))\r\n",
    "\r\n",
    "    words = nltk.word_tokenize(text)\r\n",
    "\r\n",
    "    # Reducing words to their root form\r\n",
    "    stem = PorterStemmer()\r\n",
    "\r\n",
    "    # Creating dictionary for the word frequency table\r\n",
    "    frequency_table = dict()\r\n",
    "    for wd in words:\r\n",
    "        wd = stem.stem(wd)\r\n",
    "        if wd in stop_words:\r\n",
    "            continue\r\n",
    "        if wd in frequency_table:\r\n",
    "            frequency_table[wd] += 1\r\n",
    "        else:\r\n",
    "            frequency_table[wd] = 1\r\n",
    "\r\n",
    "    return frequency_table\r\n",
    "\r\n",
    "\r\n",
    "def _calculate_sentence_scores(sentences: List[str], frequency_table: dict) -> dict:\r\n",
    "\r\n",
    "    # Algorithm for scoring a sentence by its words\r\n",
    "    sentence_weight = dict()\r\n",
    "\r\n",
    "    for sentence in sentences:\r\n",
    "        # sentence_wordcount = (len(nltk.word_tokenize(sentence)))\r\n",
    "        sentence_wordcount_without_stop_words = 0\r\n",
    "        for word_weight in frequency_table:\r\n",
    "            if word_weight in sentence.lower():\r\n",
    "                sentence_wordcount_without_stop_words += 1\r\n",
    "                if sentence[:7] in sentence_weight:\r\n",
    "                    sentence_weight[sentence[:7]\r\n",
    "                                    ] += frequency_table[word_weight]\r\n",
    "                else:\r\n",
    "                    sentence_weight[sentence[:7]\r\n",
    "                                    ] = frequency_table[word_weight]\r\n",
    "\r\n",
    "        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]\r\n",
    "                                                        ] / sentence_wordcount_without_stop_words\r\n",
    "\r\n",
    "    return sentence_weight\r\n",
    "\r\n",
    "\r\n",
    "def _calculate_average_score(sentence_weight: dict) -> int:\r\n",
    "\r\n",
    "    # Calculating the average score for the sentences\r\n",
    "    sum_values = 0\r\n",
    "    for entry in sentence_weight:\r\n",
    "        sum_values += sentence_weight[entry]\r\n",
    "\r\n",
    "    # Getting sentence average value from source text\r\n",
    "    average_score = (sum_values / len(sentence_weight))\r\n",
    "\r\n",
    "    return average_score\r\n",
    "\r\n",
    "\r\n",
    "def _get_article_summary(sentences: List[str], sentence_weight: dict, threshold: int) -> str:\r\n",
    "    sentence_counter = 0\r\n",
    "    article_summary = ''\r\n",
    "\r\n",
    "    for sentence in sentences:\r\n",
    "        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\r\n",
    "            article_summary += \" \" + sentence\r\n",
    "            sentence_counter += 1\r\n",
    "\r\n",
    "    return article_summary\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from typing import List\r\n",
    "\r\n",
    "def _get_html_text(url: str) -> str:\r\n",
    "    # Fetch the HTML content and return it as text\r\n",
    "    return requests.get(url).text\r\n",
    "\r\n",
    "def parse_html_to_paragraphs(url: str, tags: List[str]) -> str:\r\n",
    "    html_content = _get_html_text(url)\r\n",
    "\r\n",
    "    # Parsing the URL content and storing in a variable\r\n",
    "    parsed = BeautifulSoup(html_content, 'html.parser')\r\n",
    "\r\n",
    "    # Get all the content in the tas\r\n",
    "    paragraphs = parsed.find_all(tags)\r\n",
    "\r\n",
    "    content = ''\r\n",
    "\r\n",
    "    # Looping through the paragraphs and adding them to the variable\r\n",
    "    for paragraph in paragraphs:\r\n",
    "        content += paragraph.text\r\n",
    "\r\n",
    "    return content"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def summarize_url(url: str, tags: List[str] = ['p'], method: str = 'naive'):\r\n",
    "    # Get the content from the url\r\n",
    "    content = parse_html_to_paragraphs(url, tags)\r\n",
    "\r\n",
    "    # If no content is fetched\r\n",
    "    if content == \"\":\r\n",
    "        print(f\"No content of {tags} is fetched from {url}\")\r\n",
    "        sys.exit(0)\r\n",
    "\r\n",
    "    summary = \"\"\r\n",
    "\r\n",
    "    # Summarize the content\r\n",
    "    if method == \"naive\":\r\n",
    "        summary = summarize(content)\r\n",
    "\r\n",
    "    # Print the summary to stdout\r\n",
    "    print(summary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\liana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "summarize_url(\"https://wikipedia.org/wiki/Rococo\", [\"h1\", \"p\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No content of h1, p is fetched from https://wikipedia.org/wiki/Rococo\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "0",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "37833eaa7c79fc29fc64cdd65cef9244dd84f1f67a5ba8cd87f16f157512cb2c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}